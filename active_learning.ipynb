{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "active_trial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AFRWxRL2Ieb",
        "outputId": "32721a9a-e2f2-447b-dcee-b2a013c62a07"
      },
      "source": [
        "%pip install dgl\n",
        "#imports\n",
        "import os\n",
        "import random\n",
        "from random import sample\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import dgl\n",
        "import dgl.function as fn\n",
        "import dgl.nn as dglnn\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# path to directory consisting of facebook.txt\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks\") "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dgl in /usr/local/lib/python3.6/dist-packages (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from dgl) (1.18.5)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.6/dist-packages (from dgl) (2.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from dgl) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.1->dgl) (4.4.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXuxgqMM2Pt6",
        "outputId": "ad537679-19ab-43d0-a308-665074138a08"
      },
      "source": [
        "#facebook graph\n",
        "# src_dst=np.loadtxt(\"facebook.txt\",dtype=\"int32\")\n",
        "torch.manual_seed(0)\n",
        "\n",
        "src_dst = pd.read_csv(\"cora.cites\", sep='\\t', header=None, names=[\"target\", \"source\"]).to_numpy()\n",
        "\n",
        "src=src_dst[:,0]\n",
        "dst=src_dst[:,1]\n",
        "\n",
        "u = np.sort(list(set(np.concatenate([src, dst]))))\n",
        "# print(u)\n",
        "cur_ind = 0\n",
        "node_index = {}\n",
        "\n",
        "for elem in u:\n",
        "  node_index[elem] = cur_ind\n",
        "  cur_ind += 1\n",
        "\n",
        "src = np.array([node_index[x] for x in src])\n",
        "dst = np.array([node_index[x] for x in dst])\n",
        "# x = np.array([35, 40, 114])\n",
        "# print(node_index[x])\n",
        "# print(np.unique(u))\n",
        "# print(len(np.unique(u)))\n",
        "# Edges are directional in DGL; Make them bi-directional.\n",
        "u = np.concatenate([src, dst])\n",
        "v = np.concatenate([dst, src])\n",
        "# Construct a DGLGraph\n",
        "G = dgl.graph((u, v))\n",
        "G_x = G.to_networkx().to_undirected()\n",
        "\n",
        "print('We have %d nodes.' % G.number_of_nodes())\n",
        "print('We have %d edges.' % G.number_of_edges())\n",
        "print(G.nodes())\n",
        "# print(G.edges())\n",
        "# print(G_x[0])\n",
        "\n",
        "private_nodes = np.array([0, 28, 15, 71, 84, 27, 72, 47, 87, 13, 52, 198, 333, 809, 279, 3, 48, 68, 543, 131])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 2708 nodes.\n",
            "We have 10858 edges.\n",
            "tensor([   0,    1,    2,  ..., 2705, 2706, 2707])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1befhH02Zix"
      },
      "source": [
        "def find_nbr_nonnbr(G):\n",
        "    \"\"\"\n",
        "    A routine that processes a networkx graph and emits list of neighbours and non-neighbours for each node.\n",
        "    Input: NetworkX graph\n",
        "    Returns: dictionary of neighbour and non-neighbors\n",
        "    Do not use on large graphs since non-neighbour dictionary is O(n^3) storage, n: number of vertices. \n",
        "    \"\"\"\n",
        "    \n",
        "    vertex_set  = set(G.nodes())\n",
        "    vertex_list = list(vertex_set)\n",
        "    \n",
        "    nbr_dict, nonnbr_dict = {}, {}\n",
        "\n",
        "    for node in range(len(vertex_list)):\n",
        "        nbr_set = set([nbr for nbr in G[node]])\n",
        "        nonnbr_set = list(vertex_set - nbr_set)\n",
        "\n",
        "        nbr_dict[node] = nbr_set\n",
        "        nonnbr_dict[node] = nonnbr_set\n",
        "\n",
        "    return nbr_dict, nonnbr_dict\n",
        "\n",
        "class Graph:\n",
        "    def __init__(self, filename=''):\n",
        "        \"\"\"\n",
        "        Initialize a NetworkX graph from a file with edge list.\n",
        "        Raises Exception if provided file is not an edge list\n",
        "        \"\"\"\n",
        "        # G = nx.read_edgelist(filename)\n",
        "        # self.GG = G\n",
        "        # self.G = nx.convert_node_labels_to_integers(G)\n",
        "#         self.G = nx.DiGraph(self.G)\n",
        "        G = dgl.graph((u, v))\n",
        "        self.G_x = G.to_networkx().to_undirected()\n",
        "        self.vertex_set = set(self.G_x.nodes())\n",
        "        self.vertex_list = list(self.vertex_set)\n",
        "        \n",
        "    def split_train_test_private(self, test_fraction, prv_fraction=0.7):\n",
        "        \"\"\"\n",
        "        Prepares the graph for training by creating a train, test graph with non-overlapping edges \n",
        "        Input test_fraction: Fraction of neighbours per node that make the test split.\n",
        "        Returns: None\n",
        "        Adds to the self test_edges_list, train_edges_list both of which are list of triples (in, out, edge-type)\n",
        "        A new graph with edges from test omitted is attached to self called G_train. \n",
        "        \"\"\"\n",
        "        assert test_fraction<=1 and test_fraction>=0\n",
        "\n",
        "        self.test_fraction = test_fraction\n",
        "        \n",
        "        nbr_dict, nonnbr_dict = find_nbr_nonnbr(self.G_x)\n",
        "        self.nbr_dict, self.nonnbr_dict = nbr_dict, nonnbr_dict\n",
        "\n",
        "        # print(nbr_dict[0])\n",
        "        ## per_node_private_set consists of both private edges and non_edges-> non_empty only for private_nodes\n",
        "        per_node_train_set, per_node_test_set, per_node_private_set = {}, {}, {}  \n",
        "        oracle = {}         \n",
        "        test_edges_list, train_edges_list = [], []        \n",
        "        for node in range(len(self.vertex_list)):            \n",
        "            per_node_test_set[node], per_node_train_set[node], per_node_private_set[node] = {}, {}, {}\n",
        "            \n",
        "            x_nbr = int(test_fraction*len(nbr_dict[node]))\n",
        "            x_nonnbr = int(test_fraction*len(nonnbr_dict[node]))\n",
        "            x_prv_nbr = int(prv_fraction*len(nbr_dict[node]))\n",
        "            x_prv_nonnbr = int(0.25 * len(nonnbr_dict[node]))\n",
        "            \n",
        "#             print(x_nbr)\n",
        "            \n",
        "            per_node_test_set[node]['nbr'] = sample(nbr_dict[node], x_nbr)\n",
        "            if node in private_nodes:\n",
        "              # print('In')\n",
        "              # print(nbr_dict[node])\n",
        "              # print(per_node_test_set[node]['nbr'])\n",
        "              per_node_private_set[node]['nbr'] =  sample(list(set(nbr_dict[node]) - set(per_node_test_set[node]['nbr'])), x_prv_nbr)\n",
        "              for nbr in per_node_private_set[node]['nbr']:\n",
        "                oracle[(node,nbr)] = 1\n",
        "            else:\n",
        "              per_node_private_set[node]['nbr'] = []\n",
        "            per_node_train_set[node]['nbr'] =  list((set(nbr_dict[node])\\\n",
        "                                                       - set(per_node_test_set[node]['nbr'])) - set(per_node_private_set[node]['nbr']))\n",
        "            ## debug statement\n",
        "            # if node in private_nodes:\n",
        "            #   print(node, len(per_node_train_set[node]['nbr']), len(per_node_private_set[node]['nbr']), len(per_node_test_set[node]['nbr']))\n",
        "    \n",
        "            per_node_test_set[node]['nonnbr'] = sample(nonnbr_dict[node], x_nonnbr)\n",
        "            if node in private_nodes:\n",
        "              per_node_private_set[node]['nonnbr'] =  sample(list(set(nonnbr_dict[node]) - set(per_node_test_set[node]['nonnbr'])), x_prv_nonnbr)\n",
        "              for nonnbr in per_node_private_set[node]['nonnbr']:\n",
        "                oracle[(node,nonnbr)] = 0\n",
        "            else:\n",
        "              per_node_private_set[node]['nonnbr'] = []\n",
        "            per_node_train_set[node]['nonnbr'] =  list((set(nonnbr_dict[node])\\\n",
        "                                                  - set(per_node_test_set[node]['nonnbr'])) - set(per_node_private_set[node]['nonnbr']))\n",
        "            \n",
        "    \n",
        "            test_edges_per_node = [(node, x) for x in per_node_test_set[node]['nbr']]\n",
        "            test_non_edges_per_node  = [(node, x) for x in per_node_test_set[node]['nonnbr']]\n",
        "            train_edges_per_node = [(node, x) for x in per_node_train_set[node]['nbr']]\n",
        "            train_non_edges_per_node  = [(node, x) for x in per_node_train_set[node]['nonnbr']]\n",
        "            \n",
        "            test_edges_list.extend([(a, b, 1) for a, b in test_edges_per_node])\n",
        "            test_edges_list.extend([(a, b, 0) for a, b in test_non_edges_per_node])\n",
        "\n",
        "            train_edges_list.extend([(a, b, 1) for a, b in train_edges_per_node])\n",
        "            train_edges_list.extend([(a, b, 0) for a, b in train_non_edges_per_node])\n",
        "\n",
        "        # print('Out')\n",
        "            \n",
        "        self.test_edges_list = test_edges_list         \n",
        "        self.train_edges_list = train_edges_list\n",
        "\n",
        "        self.oracle = oracle\n",
        "        \n",
        "        self.test_edges_per_node = per_node_test_set\n",
        "        self.test_non_edges_per_node = test_non_edges_per_node\n",
        "        self.train_edges_per_node = per_node_train_set\n",
        "        # self.test_non_edges_per_node = test_non_edges_per_node\n",
        "\n",
        "        self.private_edges_per_node = per_node_private_set\n",
        "        \n",
        "        # G_train =  copy.deepcopy(self.G)\n",
        "        # G_train.remove_edges([(a, b) for (a, b, label) in test_edges_list if label==1])\n",
        "        train_edges_list_1 = [(a,b) for (a,b,label) in train_edges_list if label==1]\n",
        "\n",
        "        edges_src = [a for (a,b) in train_edges_list_1]\n",
        "        edges_dst = [b for (a,b) in train_edges_list_1]\n",
        "        # print(edges_src)\n",
        "        G_train = dgl.graph((edges_src, edges_dst))\n",
        "\n",
        "        self.G_train = G_train"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "j67qBuwW2kna",
        "outputId": "c3a86520-e01f-467f-872b-b5b735a6f7d6"
      },
      "source": [
        "'''\n",
        "CREATING ORIGINAL GRAPH (FOR TRAINING)\n",
        "'''\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "g = Graph()\n",
        "g.split_train_test_private(test_fraction = 0.2)\n",
        "graph = g.G_train\n",
        "\n",
        "val = 812\n",
        "print((28, val, 1) in g.train_edges_list)\n",
        "print((28, val, 1) in g.test_edges_list)\n",
        "print((28,val) in g.oracle.keys())\n",
        "# print(g.oracle[(28,val)])\n",
        "\n",
        "nx_G = graph.to_networkx().to_undirected()\n",
        "degree_sequence = sorted([d for n, d in nx_G.degree()], reverse=True)  # degree sequence\n",
        "degreeCount = collections.Counter(degree_sequence)\n",
        "deg, cnt = zip(*degreeCount.items())\n",
        "# for i in range(len(deg)):\n",
        "    # print(deg[i], cnt[i])\n",
        "# print(sorted([(x,y) for (x,y) in nx_G.degree() if y>=15], key=lambda x: x[1], reverse=True))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "plt.bar(deg, cnt, width=0.80, color=\"b\")\n",
        "\n",
        "plt.title(\"Degree Histogram\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xlabel(\"Degree\")\n",
        "ax.set_xticks([d + 2 for d in deg])\n",
        "ax.set_xticklabels(deg)\n",
        "\n",
        "priv_cnt = 0\n",
        "for (key, val) in g.oracle.items():\n",
        "  if val==1:\n",
        "    # print(key, val)\n",
        "    priv_cnt += 1\n",
        "\n",
        "print('Private nodes = ' + str(len(private_nodes)))\n",
        "# print('No. of training edges = '+str(len(g.train_edges_list)))\n",
        "# print('No. of test edges = ' + str(len(g.test_edges_list)))\n",
        "print('No. of private edges = ' + str(priv_cnt))\n",
        "\n",
        "print(len(graph.edges()[0]))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "False\n",
            "False\n",
            "Private nodes = 20\n",
            "No. of private edges = 522\n",
            "9047\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgED1bUO2nTp",
        "outputId": "22b560e2-9d65-4489-9f8e-15051fd12af5"
      },
      "source": [
        "## initializing 10D features for each node in the graph\n",
        "graph.ndata['feat'] = torch.tensor(pd.read_csv(\"cora.content\", sep='\\t', header=None).drop([0, 1434], axis=1).to_numpy())\n",
        "\n",
        "no_of_nodes = len(g.vertex_list)\n",
        "\n",
        "neg_src, neg_dst = [], []\n",
        "\n",
        "'''\n",
        "CREATING NEGATIVE GRAPH (FOR TRAINING)\n",
        "\n",
        "negative_graph is a graph whose edges are a subset of non_edges of the original graph\n",
        "(subset of complement graph).\n",
        "This is required to reduce the complexity of pairwise hinge-loss (ranking-loss) as\n",
        "no. of non-edges >> no. of edges\n",
        "'''\n",
        "\n",
        "for node in g.vertex_list:\n",
        "  all_neg = g.train_edges_per_node[node]['nonnbr']\n",
        "  sample_size = int(0.2 * len(all_neg))\n",
        "  sample_neg = np.random.choice(all_neg, size=sample_size, replace=False)\n",
        "  neg_src += [node for _ in sample_neg]\n",
        "  neg_dst += [node for node in sample_neg]\n",
        "\n",
        "negative_graph = dgl.graph((neg_src, neg_dst), num_nodes=no_of_nodes)\n",
        "\n",
        "nx_G = graph.to_networkx().to_undirected()\n",
        "nx_Gm = negative_graph.to_networkx().to_undirected()\n",
        "# Kamada-Kawaii layout usually looks pretty for arbitrary graphs\n",
        "# pos = nx.kamada_kawai_layout(nx_G)\n",
        "# nx.draw(nx_G, pos, with_labels=True, node_color=[[.7, .7, .7]])\n",
        "# print(nx_G.edges())\n",
        "# print(nx_Gm.edges())\n",
        "# list(set(nx_G.edges()) & set(nx_Gm.edges())) \n",
        "\n",
        "## DIAGRAM OF ORIGINAL GRAPH\n",
        "print(len(negative_graph.edges()[0]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1168003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le8GeGU52qrW"
      },
      "source": [
        "'''\n",
        "CREATING TEST GRAPH (FOR INFERENCE)\n",
        "\n",
        "test_graph is a graph whose edges are the test_edges and test_non_edges\n",
        "of the original graph. \n",
        "\n",
        "There is a label (1/0) associated with each edge in test_graph, indicating whether\n",
        "the edge is an edge/non-edge of the original graph.\n",
        "\n",
        "Such a non-intuitive graph creation is necessary because during inference, we need to find\n",
        "score for all node-pairs in the test_set (for ranking) and creating a graph with all\n",
        "such node-pairs (to be ranked) as edges, makes it an easy and efficient routine in DGL.\n",
        "'''\n",
        "\n",
        "N = len(graph.nodes())\n",
        "test_src, test_dst, edge_label = [], [], []\n",
        "\n",
        "for node in range(N):\n",
        "  all_nbr = g.test_edges_per_node[node]['nbr']\n",
        "  test_src += [node for _ in all_nbr]\n",
        "  test_dst += [node for node in all_nbr]\n",
        "  edge_label += [1 for _ in all_nbr]\n",
        "\n",
        "  all_nonnbr = g.test_edges_per_node[node]['nonnbr']\n",
        "  test_src += [node for _ in all_nonnbr]\n",
        "  test_dst += [node for node in all_nonnbr]\n",
        "  edge_label += [0 for _ in all_nonnbr]\n",
        "\n",
        "test_graph = dgl.graph((test_src, test_dst), num_nodes=no_of_nodes)\n",
        "test_graph.edata['label'] = torch.tensor(edge_label)\n",
        "\n",
        "nx_Gtest = test_graph.to_networkx().to_undirected()\n",
        "# Kamada-Kawaii layout usually looks pretty for arbitrary graphs\n",
        "# pos = nx.kamada_kawai_layout(nx_Gtest)\n",
        "# nx.draw(nx_Gtest, pos, with_labels=True, node_color=[[.7, .7, .7]])\n",
        "\n",
        "## DIAGRAM OF TEST GRAPH"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vHMCVjT2uPC"
      },
      "source": [
        "'''\n",
        "CREATING PRIVATE GRAPH (FOR FINDING UNCERTAIN EDGES BEFORE ORACLE-QUERY)\n",
        "\n",
        "private_graph is a graph whose edges are the private node-pairs\n",
        "of the original graph. \n",
        "'''\n",
        "\n",
        "private_src, private_dst = [], []\n",
        "\n",
        "for node in private_nodes:\n",
        "  all_nbr = g.private_edges_per_node[node]['nbr']\n",
        "  private_src += [node for _ in all_nbr]\n",
        "  private_dst += [node for node in all_nbr]\n",
        "\n",
        "  all_nonnbr = g.private_edges_per_node[node]['nonnbr']\n",
        "  private_src += [node for _ in all_nonnbr]\n",
        "  private_dst += [node for node in all_nonnbr]\n",
        "\n",
        "private_graph = dgl.graph((private_src, private_dst), num_nodes=no_of_nodes)\n",
        "# test_graph.edata['label'] = torch.tensor(edge_label)\n",
        "\n",
        "nx_Gpriv = private_graph.to_networkx().to_undirected()\n",
        "# Kamada-Kawaii layout usually looks pretty for arbitrary graphs\n",
        "# pos = nx.kamada_kawai_layout(nx_Gpriv)\n",
        "# nx.draw(nx_Gpriv, pos, with_labels=True, node_color=[[.7, .7, .7]])\n",
        "\n",
        "## DIAGRAM OF PRIVATE GRAPH"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04pXETAt2wxF"
      },
      "source": [
        "'''\n",
        "MODEL DEFINED HERE\n",
        "\n",
        "GraphSAGE is for computing embeddings for each node.\n",
        "\n",
        "DotProduct takes those embeddings and computes edge_embeddings for all edges\n",
        "in the graph (takes as argument in forward(...)). Note this may not be the training graph.\n",
        "Now it is clear, why we created separate negative_graph and test_graph.\n",
        "\n",
        "Model takes 2 graphs, computes SAGE embeddings using former graph and returns scores\n",
        "for edges in both former and later graph.\n",
        "\n",
        "For reference:-\n",
        "https://docs.dgl.ai/guide/training-link.html\n",
        "\n",
        "In this implementation, input-features are 10D, output-features (embeddings) are 50D.\n",
        "score(u,v) = np.dot(embedding(u), embedding(v)), which is a real number\n",
        "'''\n",
        "\n",
        "gcn_msg = fn.copy_src(src='h', out='m')\n",
        "gcn_reduce = fn.sum(msg='m', out='h')\n",
        "\n",
        "class GCNLayer(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats):\n",
        "        super(GCNLayer, self).__init__()\n",
        "        self.linear = nn.Linear(in_feats, out_feats)\n",
        "\n",
        "    def forward(self, g, feature):\n",
        "        # Creating a local scope so that all the stored ndata and edata\n",
        "        # (such as the `'h'` ndata below) are automatically popped out\n",
        "        # when the scope exits.\n",
        "        with g.local_scope():\n",
        "            g.ndata['h'] = feature\n",
        "            g.update_all(gcn_msg, gcn_reduce)\n",
        "            h = g.ndata['h']\n",
        "            return self.linear(h)\n",
        "\n",
        "class SAGE(nn.Module):\n",
        "    def __init__(self, in_feats, hid_feats, out_feats):\n",
        "        super().__init__()\n",
        "        self.conv1 = dglnn.SAGEConv(\n",
        "            in_feats=in_feats, out_feats=hid_feats, aggregator_type='mean')\n",
        "        self.conv2 = dglnn.SAGEConv(\n",
        "            in_feats=hid_feats, out_feats=out_feats, aggregator_type='mean')\n",
        "\n",
        "    def forward(self, graph, inputs):\n",
        "        # inputs are features of nodes\n",
        "        h = self.conv1(graph, inputs)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(graph, h)\n",
        "        return h\n",
        "\n",
        "class DotProductPredictor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, graph, h):\n",
        "        # h contains the node representations computed from the GNN defined\n",
        "        # in the node classification section (Section 5.1).\n",
        "        with graph.local_scope():\n",
        "            graph.ndata['h'] = h\n",
        "            graph.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
        "            return graph.edata['score']\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, out_features):\n",
        "        super().__init__()\n",
        "        # self.layer1 = GCNLayer(1433, 16)\n",
        "        # self.layer2 = GCNLayer(16, 7)\n",
        "        self.sage = SAGE(in_features, hidden_features, out_features)\n",
        "        self.pred = DotProductPredictor()\n",
        "    def forward(self, g, neg_g, x):\n",
        "        h = self.sage(g, x)\n",
        "        # x = F.relu(self.layer1(g, x))\n",
        "        # h = self.layer2(g, x)\n",
        "        return self.pred(g, h), self.pred(neg_g, h)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reod6rmZ2za5"
      },
      "source": [
        "'''\n",
        "VALIDATION (INFERENCE) ROUTINE HERE\n",
        "\n",
        "Takes graph and score for all test node-pairs as arguments\n",
        "and computes MAP for all private nodes.\n",
        "\n",
        "Used for cross-validation during training of LP model \n",
        "'''\n",
        "\n",
        "def validation(graph, score):\n",
        "  (graph_src, graph_dst) = graph.edges()\n",
        "  edge_labels = graph.edata['label']\n",
        "\n",
        "  N = len(graph.nodes())\n",
        "\n",
        "  ranklists, labels = {}, {}\n",
        "  for node in range(N):\n",
        "    ranklists[node] = []\n",
        "\n",
        "  for i in range(len(graph_src)):\n",
        "    ranklists[graph_src[i].item()].append((score[i].item(), edge_labels[i].item()))\n",
        "    ranklists[graph_dst[i].item()].append((score[i].item(), edge_labels[i].item()))\n",
        "\n",
        "  total_score, val_ct = 0, 0\n",
        "\n",
        "  large_degree_nodes = [0, 28, 121, 71, 15, 2, 84, 328, 27, 72, 47, 122, 379, 13, 87, 173, 222, 52, 198, 333, 809, 236, 355, 279, 464, 3, 48, 64, 68, 91, 182, 543, 85, 131, 240, 509, 822, 1217, 200, 535, 642, 643]\n",
        "  \n",
        "  # sampled_nodes = np.random.choice(N, size=100, replace=False)\n",
        "  for node in large_degree_nodes:\n",
        "    ranklists[node] = np.array(sorted(ranklists[node], key = lambda x: x[0], reverse=True))\n",
        "    # print('Node = '+str(node))\n",
        "    # print(ranklists[node])\n",
        "    # print(node)\n",
        "    # print(ranklists[node])\n",
        "    ground_truth_labels = ranklists[node][:,1]\n",
        "    one_cnt = np.sum(ground_truth_labels)\n",
        "    zero_cnt = np.sum(1-ground_truth_labels)\n",
        "    if one_cnt>0 and zero_cnt>0:\n",
        "      avp = np.sum((np.cumsum(ground_truth_labels)*ground_truth_labels)/np.arange(1,len(ground_truth_labels)+1))/one_cnt\n",
        "      total_score+=avp\n",
        "      # print(node, avp)\n",
        "      val_ct+=1\n",
        "  sampled_map = total_score/val_ct\n",
        "\n",
        "  total_score, val_ct = 0, 0\n",
        "\n",
        "  for node in private_nodes:\n",
        "    ranklists[node] = np.array(sorted(ranklists[node], key = lambda x: x[0], reverse=True))\n",
        "    # print('Node = '+str(node))\n",
        "    # print(ranklists[node])\n",
        "    ground_truth_labels = ranklists[node][:,1]\n",
        "    one_cnt = np.sum(ground_truth_labels)\n",
        "    zero_cnt = np.sum(1-ground_truth_labels)\n",
        "    if one_cnt>0 and zero_cnt>0:\n",
        "      avp = np.sum((np.cumsum(ground_truth_labels)*ground_truth_labels)/np.arange(1,len(ground_truth_labels)+1))/one_cnt\n",
        "      total_score+=avp\n",
        "      # print(node, avp)\n",
        "      val_ct+=1\n",
        "  uncertain_map = total_score/val_ct\n",
        "  return sampled_map, uncertain_map"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szfogzNO228P"
      },
      "source": [
        "'''\n",
        "NODE-PAIR TO QUERY FOR ACTIVE LEARNING\n",
        "\n",
        "Objective: Find node-pair from pool-set which maximizes expected value of a separate loss.\n",
        "Note: This loss is not directly a surrogate for MAP.\n",
        "'''\n",
        "\n",
        "def select_random_pairs(model, graph, negative_graph, private_graph, node_features):\n",
        "  (priv_graph_src, priv_graph_dst) = private_graph.edges()\n",
        "  priv_pair_count = len(priv_graph_src)\n",
        "\n",
        "  indices = np.random.choice(priv_pair_count, size=40, replace=False)\n",
        "  querying_pairs = [(priv_graph_src[index].item(), priv_graph_dst[index].item(), None) for index in indices]\n",
        "\n",
        "  return querying_pairs\n",
        "\n",
        "def find_uncertain_edges(model, graph, negative_graph, private_graph, node_features):\n",
        "  pub_score, neg_score = model(graph, negative_graph, node_features)\n",
        "  pub_score, priv_score = model(graph, private_graph, node_features)\n",
        "  (pos_graph_src, pos_graph_dst) = graph.edges()\n",
        "  (neg_graph_src, neg_graph_dst) = negative_graph.edges()\n",
        "  (priv_graph_src, priv_graph_dst) = private_graph.edges()\n",
        "\n",
        "  ranklists, unc_ranklists = {}, {}\n",
        "  for node in private_nodes:\n",
        "    ranklists[node], unc_ranklists[node] = [], []\n",
        "\n",
        "  ## add the scores for labelled pairs to the ranklist of respective node\n",
        "\n",
        "  for i in range(len(pos_graph_src)):\n",
        "    if pos_graph_src[i].item() in private_nodes:\n",
        "      ranklists[pos_graph_src[i].item()].append((pub_score[i].item(), 1))\n",
        "\n",
        "  for i in range(len(neg_graph_src)):\n",
        "    if neg_graph_src[i].item() in private_nodes:\n",
        "      ranklists[neg_graph_src[i].item()].append((neg_score[i].item(), 0))\n",
        "\n",
        "   ## add the scores of uncertain pairs to the unc_ranklist of respective node \n",
        "\n",
        "  for i in range(len(priv_graph_src)):\n",
        "    if priv_graph_src[i].item() in private_nodes:\n",
        "      unc_ranklists[priv_graph_src[i].item()].append((priv_graph_dst[i].item(), priv_score[i].item()))\n",
        "\n",
        "  for node in private_nodes:\n",
        "    ranklists[node] = sorted(ranklists[node], key=lambda x: x[0], reverse=True)\n",
        "    unc_ranklists[node] = sorted(unc_ranklists[node], key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  sorted_queryable = []\n",
        "  k = int(40/len(private_nodes)) + 1\n",
        "  '''\n",
        "  For each private node, calculate the loss for nodes with 3 highest and 3 lowest scores. Because\n",
        "  of the nature of the Loss function, this heuristic is expected to works and reduces the\n",
        "  complexity from O(|V|^3) -> O(|V|^2)\n",
        "  '''\n",
        "\n",
        "  for node in private_nodes:\n",
        "    # sorted_pool = sorted(unc_ranklists[node],key= lambda x: x[1],reverse=True)\n",
        "    choices = unc_ranklists[node] #set([sorted_pool[0], sorted_pool[1], sorted_pool[2], sorted_pool[-1], sorted_pool[-2], sorted_pool[-3]])\n",
        "    # print(choices)\n",
        "    # exit()\n",
        "    pool_pair_losses = []\n",
        "\n",
        "    for (j, (cur_index, cur_score)) in enumerate(choices):\n",
        "      if cur_index == node:\n",
        "        continue\n",
        "      loss = 0\n",
        "      tmp = 0\n",
        "      for (i, (score, index)) in enumerate(unc_ranklists[node]):\n",
        "        min_pos = min(i, j) + 1\n",
        "        loss += np.abs(score - cur_score)/4\n",
        "        # print(loss)\n",
        "        tmp = loss\n",
        "      # exit() \n",
        "      for (i, (score, label)) in enumerate(ranklists[node]):\n",
        "        min_pos = min(i, j) + 1\n",
        "        if score > cur_score and label == 0:\n",
        "          loss += (score - cur_score)/2\n",
        "        elif score < cur_score and label == 1:\n",
        "          loss += (cur_score - score)/2\n",
        "      # print(node, \"intra-inter\",tmp, loss-tmp)\n",
        "      pool_pair_losses.append((node, cur_index, loss, g.oracle[(node, cur_index)]))\n",
        "\n",
        "    per_node_queryable = sorted(pool_pair_losses, key= lambda x: x[2], reverse=True)[:k]\n",
        "    # print(node, len(ranklists[node]) + len(unc_ranklists[node]), per_node_queryable[:3])\n",
        "    sorted_queryable += per_node_queryable\n",
        "  # print(sorted_queryable[:40])\n",
        "  return sorted_queryable[:40]\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IHFkZMWx26YP",
        "outputId": "87adc56f-bdbc-43d5-c9ee-3414eecbaa62"
      },
      "source": [
        "'''\n",
        "LOSS, OPTIMIZER AND TRAINING HERE\n",
        "\n",
        "loss function to compute mean pair-wise hinge loss across all (edge, non-edge) pairs\n",
        "with a common incident node.\n",
        "\n",
        "This is common form of ranking loss we have done in class (probably surrogate of AUC).\n",
        "\n",
        "Reference:-\n",
        "https://docs.dgl.ai/guide/training-link.html\n",
        "\n",
        "** Please check Line 37: + and - interchanged wrt reference. \n",
        "'''\n",
        "\n",
        "def compute_loss(pos_score, neg_score, priv_score, pos, neg, prv, N):\n",
        "    # Margin loss\n",
        "    # print(len(pos_score))\n",
        "    # print(len(neg_score))\n",
        "\n",
        "    # print(graph_src[:1000])\n",
        "    # print(neg_graph_src[:2000])\n",
        "    tot_loss = 0\n",
        "    # n_edges = pos_score.shape[0]\n",
        "\n",
        "    # sampled_nodes = np.random.choice(N, size=800, replace=False)\n",
        "\n",
        "    for i in range(N):\n",
        "      # print(i)\n",
        "      pos_indices, neg_indices, priv_indices = pos[i], neg[i], prv[i]\n",
        "      # sampled_pos_size = min(20, len(pos_indices))\n",
        "      sampled_neg_size = min(400, len(pos_indices))\n",
        "      \n",
        "      sampled_neg_indices = np.random.choice(neg_indices, size=sampled_neg_size, replace=False)\n",
        "\n",
        "      for j in pos_indices:\n",
        "        for k in sampled_neg_indices:\n",
        "          tot_loss += (1 + neg_score[k] - pos_score[j]).clamp(min=0)\n",
        "\n",
        "      if i in private_nodes:\n",
        "        priv_indices = np.random.choice(priv_indices, size=200, replace=False)\n",
        "\n",
        "        for j in priv_indices:\n",
        "          for k in pos_indices:\n",
        "            if (pos_score[k] < priv_score[j]):\n",
        "              tot_loss += (1 + priv_score[j] - pos_score[k]).clamp(min=0)/2\n",
        "\n",
        "        for j in priv_indices:\n",
        "          for k in sampled_neg_indices:\n",
        "            if (neg_score[k] > priv_score[j]):\n",
        "              tot_loss += (1 + neg_score[k] - priv_score[j]).clamp(min=0)/2\n",
        "        \n",
        "    loss_mean = tot_loss.mean()\n",
        "    # print(loss_mean)\n",
        "    return loss_mean\n",
        "\n",
        "torch.manual_seed(0)\n",
        "## Node features must be float\n",
        "node_features = graph.ndata['feat'] * 1.0\n",
        "n_features = node_features.shape[1]\n",
        "model = Model(n_features, 50, 50)\n",
        "# model = torch.load('our_model_60')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "validation_interval = 5\n",
        "\n",
        "## Budget and no. of edges queried till now\n",
        "B = 200\n",
        "edges_queried = 0\n",
        "\n",
        "## Uncomment these lines to print status\n",
        "# print('No. of edges = '+str(len(graph.edges()[0])))\n",
        "# print('No. of non-edges = '+str(len(negative_graph.edges()[0])))\n",
        "# print('No. of uncertain-pairs = '+str(len(private_graph.edges()[0])))\n",
        "\n",
        "global_epoch_counter = 0\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "\n",
        "while edges_queried <= B:\n",
        "\n",
        "  (graph_src, graph_dst) = graph.edges()\n",
        "  print('Graph edges extracted')\n",
        "  print(len(graph_src))\n",
        "  (neg_graph_src, neg_graph_dst) = negative_graph.edges()\n",
        "  print('Negative graph edges extracted')\n",
        "  print(len(neg_graph_src))\n",
        "  (priv_graph_src, priv_graph_dst) = private_graph.edges()\n",
        "  print('Private graph edges extracted')\n",
        "  print(len(priv_graph_src))\n",
        "\n",
        "  N = len(graph.nodes())\n",
        "\n",
        "  pos, neg, prv = {}, {}, {}\n",
        "\n",
        "  for i in range(N):\n",
        "    pos[i], neg[i], prv[i] = [], [], [] \n",
        "\n",
        "  for i, node in enumerate(graph_src):\n",
        "    pos[node.item()].append(i)\n",
        "  for i, node in enumerate(neg_graph_src):\n",
        "    neg[node.item()].append(i)\n",
        "  for i, node in enumerate(priv_graph_src):\n",
        "    prv[node.item()].append(i)\n",
        "\n",
        "  print('Indices for each node extracted')\n",
        "  # pos_score, test_score = model(graph, test_graph, node_features)\n",
        "  # sampled_map, uncertain_map = validation(test_graph, test_score)\n",
        "  # print('Sampled MAP = ' + str(sampled_map))\n",
        "  # print('Uncertain MAP = ' + str(uncertain_map)+'\\n')\n",
        "  \n",
        "  ## Training for 10 epochs, cross-validation after every 5 epochs after each query.\n",
        "  for epoch in range(10):\n",
        "      scheduler.step()\n",
        "      print('Learning rate = ' + str(optimizer.param_groups[0]['lr']))\n",
        "      global_epoch_counter += 1\n",
        "      print('Epoch: '+str(epoch+1))\n",
        "      # negative_graph = construct_negative_graph(graph, k)\n",
        "      \n",
        "      pos_score, neg_score = model(graph, negative_graph, node_features)\n",
        "      pub_score, priv_score = model(graph, private_graph, node_features)\n",
        "      \n",
        "      # neg_score = neg_score\n",
        "      print('model scores computed')\n",
        "      # loss = criterion(torch.cat([pos_score, neg_score]), torch.cat([torch.ones(len(pos_score), 1), torch.zeros(len(neg_score), 1)])) \n",
        "      # loss = criterion(pos_score, torch.ones(len(pos_score), 1)) * 500 + criterion(neg_score, torch.zeros(len(neg_score), 1)) \n",
        "      loss = compute_loss(pos_score, neg_score, priv_score, pos, neg, prv, N)\n",
        "      # loss = compute_loss(pos_score, neg_score)\n",
        "      print('Loss calculated')\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      # print('loss.backward() evaluated')\n",
        "      optimizer.step()\n",
        "      print(loss.item())\n",
        "\n",
        "      if (epoch+1) % validation_interval == 0:\n",
        "        torch.save(model, '/content/drive/My Drive/Colab Notebooks/model_' + str(global_epoch_counter))\n",
        "        print('Epoch: ' + str(global_epoch_counter))\n",
        "        pos_score, test_score = model(graph, test_graph, node_features)\n",
        "        sampled_map, uncertain_map = validation(test_graph, test_score)\n",
        "        print('Selected MAP = ' + str(sampled_map))\n",
        "        print('Uncertain MAP = ' + str(uncertain_map)+'\\n')\n",
        "\n",
        "  # uncertain_pairs = select_random_pairs(model, graph, negative_graph, private_graph, node_features)\n",
        "  uncertain_pairs = find_uncertain_edges(model, graph, negative_graph, private_graph, node_features)\n",
        "  # print(uncertain_pairs)\n",
        "  pos_src, pos_dst, neg_src, neg_dst, total_pairs = [], [], [], [], []\n",
        "\n",
        "  '''\n",
        "  Based on node-pairs chosen:-\n",
        "  1. query the oracle\n",
        "  2. remove the node-pairs from the private_graph\n",
        "  3. if the oracle returns 1, add the node-pair to graph\n",
        "  4  if the oracle returns 0, add the node-pair to negative_graph \n",
        "  '''\n",
        "\n",
        "  for (u,v,loss) in uncertain_pairs:\n",
        "    label = g.oracle[(u,v)]\n",
        "    total_pairs.append((u,v))\n",
        "    if label == 1:\n",
        "      pos_src.append(u)\n",
        "      pos_dst.append(v)\n",
        "    else:\n",
        "      neg_src.append(u)\n",
        "      neg_dst.append(v)\n",
        "    edges_queried += 1\n",
        "\n",
        "  print('Oracle queries made')\n",
        "    \n",
        "  graph = dgl.add_edges(graph, pos_src, pos_dst)\n",
        "  negative_graph = dgl.add_edges(negative_graph, neg_src, neg_dst)\n",
        "\n",
        "  private_graph_nx = private_graph.to_networkx()\n",
        "  private_graph_nx.remove_edges_from(total_pairs)\n",
        "  private_graph = dgl.from_networkx(private_graph_nx)\n",
        "\n",
        "  # Uncomment these lines to check if graphs modified properly after query\n",
        "  print('query status = ' +str(edges_queried)+'/'+str(B))\n",
        "  print('No. of edges = '+str(len(graph.edges()[0])))\n",
        "  print('No. of non-edges = '+str(len(negative_graph.edges()[0])))\n",
        "  print('No. of uncertain-pairs = '+str(len(private_graph.edges()[0])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Graph edges extracted\n",
            "9047\n",
            "Negative graph edges extracted\n",
            "1168003\n",
            "Private graph edges extracted\n",
            "13866\n",
            "Indices for each node extracted\n",
            "Learning rate = 0.005\n",
            "Epoch: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "model scores computed\n",
            "Loss calculated\n",
            "269546.03125\n",
            "Learning rate = 0.005\n",
            "Epoch: 2\n",
            "model scores computed\n",
            "Loss calculated\n",
            "180563.9375\n",
            "Learning rate = 0.005\n",
            "Epoch: 3\n",
            "model scores computed\n",
            "Loss calculated\n",
            "164454.484375\n",
            "Learning rate = 0.005\n",
            "Epoch: 4\n",
            "model scores computed\n",
            "Loss calculated\n",
            "158617.09375\n",
            "Learning rate = 0.005\n",
            "Epoch: 5\n",
            "model scores computed\n",
            "Loss calculated\n",
            "155491.578125\n",
            "Epoch: 5\n",
            "tensor([[0.3374],\n",
            "        [0.2956],\n",
            "        [0.3186],\n",
            "        [0.3229],\n",
            "        [0.3016]], grad_fn=<SliceBackward>)\n",
            "tensor([[0.3446],\n",
            "        [0.3089],\n",
            "        [0.3298],\n",
            "        [0.2401],\n",
            "        [0.3525]], grad_fn=<SliceBackward>)\n",
            "Selected MAP = 0.02599838375700548\n",
            "Uncertain MAP = 0.02604857460066153\n",
            "\n",
            "Learning rate = 0.005\n",
            "Epoch: 6\n",
            "model scores computed\n",
            "Loss calculated\n",
            "152860.28125\n",
            "Learning rate = 0.005\n",
            "Epoch: 7\n",
            "model scores computed\n",
            "Loss calculated\n",
            "151425.9375\n",
            "Learning rate = 0.005\n",
            "Epoch: 8\n",
            "model scores computed\n",
            "Loss calculated\n",
            "150257.96875\n",
            "Learning rate = 0.005\n",
            "Epoch: 9\n",
            "model scores computed\n",
            "Loss calculated\n",
            "149272.109375\n",
            "Learning rate = 0.005\n",
            "Epoch: 10\n",
            "model scores computed\n",
            "Loss calculated\n",
            "147866.1875\n",
            "Epoch: 10\n",
            "tensor([[0.3135],\n",
            "        [0.3008],\n",
            "        [0.3108],\n",
            "        [0.3196],\n",
            "        [0.3009]], grad_fn=<SliceBackward>)\n",
            "tensor([[0.2199],\n",
            "        [0.3008],\n",
            "        [0.3286],\n",
            "        [0.3034],\n",
            "        [0.3211]], grad_fn=<SliceBackward>)\n",
            "Selected MAP = 0.045784227841403714\n",
            "Uncertain MAP = 0.042491199997044725\n",
            "\n",
            "(0, 1913, 265851.03845384344, 0)\n",
            "(28, 1566, 243486.03149036318, 0)\n",
            "(15, 2101, 236013.1568641737, 0)\n",
            "(71, 1913, 238321.85120107466, 0)\n",
            "(84, 2324, 237403.50488420203, 0)\n",
            "(27, 1250, 236339.60567960888, 0)\n",
            "(72, 1913, 235074.39169434924, 0)\n",
            "(47, 1250, 237216.61904443055, 0)\n",
            "(87, 1910, 222218.91132970527, 0)\n",
            "(13, 1913, 231663.44935017265, 0)\n",
            "(52, 2012, 237378.77700266242, 0)\n",
            "(198, 1566, 238295.15968552604, 0)\n",
            "(333, 2138, 235725.78408201784, 0)\n",
            "(809, 1913, 240306.33087544056, 0)\n",
            "(279, 1913, 229650.1511549689, 0)\n",
            "(3, 1864, 226294.18380202912, 0)\n",
            "(48, 2396, 236069.30896753073, 0)\n",
            "(68, 2396, 231278.91220658273, 0)\n",
            "(543, 403, 228040.15369394794, 0)\n",
            "(131, 1894, 233717.91079791635, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e1024baad517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m   '''\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muncertain_pairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mtotal_pairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
          ]
        }
      ]
    }
  ]
}